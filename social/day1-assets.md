## Day‑1 campaign assets

Post copy (Page/Profile)
- Hook: Fluency isn’t understanding. We evaluate open‑ended LLM reasoning with deterministic, binary checks.
- Body: This month we’ll share how Correctness, Completeness, and Groundedness expose failures multiple‑choice benchmarks miss—and how small models can out‑reason bigger ones under constraints.
- CTA: What single binary gate would improve your agent today?
- Optional link: https://eqlabs.ai/index.html?utm_source=facebook&utm_medium=social&utm_campaign=30d-deterministic&utm_content=day1-hero

Comment scripts (pick 5–8)
- How do you score beyond ROUGE/BLEU? Binary checks (Correctness/Completeness/Groundedness) gave us clearer decisions. What’s your ship gate?
- Do you track groundedness% per dataset slice? It reduced hallucinations more than prompt tweaks for us.
- Before agent tool use, do you require a completeness pass? A simple gate cut dead‑end branches.
- What’s your reproducibility setup—dataset versioning + flags per trial or ad‑hoc eval?
- Seen small models beat larger on grounded answers? What constraints made it work?

Checklist (today)
- [ ] Publish hero post
- [ ] 10–12 high‑signal comments across 5–7 groups
- [ ] 2–3 DMs to mods/collabs
- [ ] Log metrics in tracker


