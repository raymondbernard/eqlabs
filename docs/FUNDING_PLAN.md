## Funding plan (EQLabs 2025)

### Objectives
- Secure a mix of sponsorships (cash + in‑kind compute), paid pilots, and ecosystem partnerships to fund EQUATOR benchmark workstreams.
- Maintain independence and scientific rigor via blind studies, binary criteria, and reproducible artifacts.

### Funding tracks
- Corporate sponsorships (frontier labs, platforms, MLOps): cash sponsorship, co‑branded benchmarks, and research advisory seats.
- Compute programs (hyperscalers + silicon): credits/GPUs for benchmark runs and public leaderboards.
- Enterprise pilots (regulated/high‑stakes): scoped paid evaluations with policy‑grade reporting.
- Ecosystem integrations (agent frameworks/OSS): native "eval packs" and badges that drive usage + visibility.

### Sponsorship tiers (indicative)
- Silver: logo on benchmark page + quarterly briefing; light co‑marketing.
- Gold: above + co‑branded case study + early access to findings.
- Platinum: above + dedicated blind‑study track + advisory seat on evaluation topics.

### Initial target categories
- Frontier labs: OpenAI, Anthropic, Google DeepMind, Meta FAIR, xAI, Cohere, Mistral
- Cloud & platforms: AWS, Azure, Google Cloud, Oracle, Databricks, Snowflake, Hugging Face
- Safety/eval/MLOps: Scale AI, LangSmith, W&B, Robust Intelligence, Arize, Arthur, Humanloop, Lakera
- Hardware: NVIDIA (Inception), AMD, Intel, Qualcomm
- Enterprise (regulated): JPMorgan AI Research, Palantir, Thomson Reuters, McKinsey QuantumBlack, BCG X

### Timeline (12 weeks)
- Weeks 1–2: pre‑brief 15–25 targets under embargo; confirm 5–8 live opportunities.
- Weeks 3–4: send 2‑pager + 90‑sec demo; propose sponsorship tiers or pilot SOWs.
- Weeks 5–8: run 2–4 pilots; publish one anonymized interim insight.
- Weeks 9–12: finalize 2+ sponsorships, 1+ compute grant, 1+ enterprise pilot renewal; publish public leaderboard.

### Deliverables by track
- Sponsorships: signed LOAs; logo/brand approvals; public benchmark placement.
- Compute: credit/GPU allocation; CI configuration for runs; reporting cadence.
- Pilots: SOW with success criteria, data handling, and report template.
- Ecosystem: integration guide, eval pack repo, badge criteria.

### Required assets (owners)
- 2‑pager + press kit (PMM)
- 90‑sec demo + offline fallback (Eng/Research)
- Public benchmark page + leaderboard (Eng)
- Reproducibility bundle (protocol + logs) (Research)

### Measurement
- 2+ paid sponsors; 1+ compute grant; 2+ pilots in flight; monthly inbound from integrations.

### Notes
- Preserve vendor‑neutral stance; disclose sponsorships; keep methodology public and reproducible.


